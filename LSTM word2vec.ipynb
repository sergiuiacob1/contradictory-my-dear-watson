{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12120"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6870"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df[train_df['language'] == 'English']\n",
    "test_df = test_df[test_df['language'] == 'English']\n",
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame()\n",
    "temp['premise'] = train_df['premise']\n",
    "temp['hypothesis'] = train_df['hypothesis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sergiuiacob/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = nltk.corpus.stopwords.words()\n",
    "\n",
    "def clean_sentence(val):\n",
    "    regex = re.compile('([^\\s\\w]|_)+')\n",
    "    sentence = regex.sub('', val).lower()\n",
    "    sentence = sentence.split(\" \")\n",
    "    \n",
    "    for word in list(sentence):\n",
    "        if word in STOP_WORDS:\n",
    "            sentence.remove(word)  \n",
    "            \n",
    "    sentence = \" \".join(sentence)\n",
    "    return sentence\n",
    "\n",
    "temp['premise'] =  temp['premise'].apply(clean_sentence)\n",
    "temp['hypothesis'] =  temp['hypothesis'].apply(clean_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_corpus(data):\n",
    "    corpus = []\n",
    "    for col in ['premise', 'hypothesis']:\n",
    "        for sentence in data[col].iteritems():\n",
    "            word_list = sentence[1].split(\" \")\n",
    "            corpus.append(word_list)\n",
    "            \n",
    "    return corpus\n",
    "\n",
    "corpus = build_corpus(temp)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(model, word_limit=100):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in list(model.wv.key_to_index)[:word_limit]:\n",
    "        tokens.append(model.wv[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(corpus, vector_size=100, window=20, min_count=10, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot(model, word_limit=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import os\n",
    "# import math\n",
    "# import subprocess\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from tqdm import tqdm\n",
    "# import random\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "# #Initialise the random seeds\n",
    "# def random_init(**kwargs):\n",
    "#     random.seed(kwargs['seed'])\n",
    "#     torch.manual_seed(kwargs['seed'])\n",
    "#     torch.cuda.manual_seed(kwargs['seed'])\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# def normalise(text):\n",
    "#     chars = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n",
    "#     text = text.upper()\n",
    "#     words=[]\n",
    "#     for w in text.strip().split():\n",
    "#         if w.startswith('HTTP'):\n",
    "#             continue\n",
    "#         while len(w)>0 and w[0] not in chars:\n",
    "#             w = w[1:]\n",
    "#         while len(w)>0 and w[-1] not in chars:\n",
    "#             w = w[:-1]\n",
    "#         if len(w) == 0:\n",
    "#             continue\n",
    "#         words.append(w)\n",
    "#     text=' '.join(words)\n",
    "#     return text\n",
    "\n",
    "# def read_vocabulary(train_text, **kwargs):\n",
    "#     vocab = dict()\n",
    "#     counts = dict()\n",
    "#     num_words = 0\n",
    "#     for line in train_text:\n",
    "#         line = (list(line.strip()) if kwargs['characters'] else line.strip().split())\n",
    "#         for char in line:\n",
    "#             if char not in vocab:\n",
    "#                 vocab[char] = num_words\n",
    "#                 counts[char] = 0\n",
    "#                 num_words+=1\n",
    "#             counts[char] += 1\n",
    "#     num_words = 0\n",
    "#     vocab2 = dict()\n",
    "#     if not kwargs['characters']:\n",
    "#         for w in vocab:\n",
    "#             if counts[w] >= args['min_count']:\n",
    "#                 vocab2[w] = num_words\n",
    "#                 num_words += 1\n",
    "#     vocab = vocab2\n",
    "#     for word in [kwargs['start_token'],kwargs['end_token'],kwargs['unk_token']]:\n",
    "#         if word not in vocab:\n",
    "#             vocab[word] = num_words\n",
    "#             num_words += 1\n",
    "#     return vocab\n",
    "\n",
    "# def load_data(premise, hypothesis, targets=None, cv=False, **kwargs):\n",
    "#     assert len(premise) == len(hypothesis)\n",
    "#     num_seq = len(premise)\n",
    "#     max_words = max([len(t) for t in premise+hypothesis])+2\n",
    "#     dataset = len(kwargs['vocab'])*torch.ones((2,max_words,num_seq),dtype=torch.long)\n",
    "#     labels = torch.zeros((num_seq),dtype=torch.uint8)\n",
    "#     idx = 0\n",
    "#     utoken_value = kwargs['vocab'][kwargs['unk_token']]\n",
    "#     for i,line in tqdm(enumerate(premise),desc='Allocating data memory',disable=(kwargs['verbose']<2)):\n",
    "#         words = (list(line.strip()) if kwargs['characters'] else line.strip().split())\n",
    "#         if len(words)==0 or words[0] != kwargs['start_token']:\n",
    "#             words.insert(0,kwargs['start_token'])\n",
    "#         if words[-1] != kwargs['end_token']:\n",
    "#             words.append(kwargs['end_token'])\n",
    "#         for jdx,word in enumerate(words):\n",
    "#             dataset[0,jdx,idx] = kwargs['vocab'].get(word,utoken_value)\n",
    "#         line=hypothesis[i]\n",
    "#         words = (list(line.strip()) if kwargs['characters'] else line.strip().split())\n",
    "#         if len(words)==0 or words[0] != kwargs['start_token']:\n",
    "#             words.insert(0,kwargs['start_token'])\n",
    "#         if words[-1] != kwargs['end_token']:\n",
    "#             words.append(kwargs['end_token'])\n",
    "#         for jdx,word in enumerate(words):\n",
    "#             dataset[1,jdx,idx] = kwargs['vocab'].get(word,utoken_value)\n",
    "#         if targets is not None:\n",
    "#             labels[idx] = targets[i]\n",
    "#         idx += 1\n",
    "\n",
    "#     if cv == False:\n",
    "#         return dataset, labels\n",
    "\n",
    "#     idx = [i for i in range(num_seq)]\n",
    "#     random.shuffle(idx)\n",
    "#     trainset = dataset[:,:,idx[0:int(num_seq*(1-kwargs['cv_percentage']))]]\n",
    "#     trainlabels = labels[idx[0:int(num_seq*(1-kwargs['cv_percentage']))]]\n",
    "#     validset = dataset[:,:,idx[int(num_seq*(1-kwargs['cv_percentage'])):]]\n",
    "#     validlabels = labels[idx[int(num_seq*(1-kwargs['cv_percentage'])):]]\n",
    "#     return trainset, validset, trainlabels, validlabels\n",
    "\n",
    "# class LSTMEncoder(nn.Module):\n",
    "#     def __init__(self, **kwargs):\n",
    "        \n",
    "#         super(LSTMEncoder, self).__init__()\n",
    "#         #Base variables\n",
    "#         self.vocab = kwargs['vocab']\n",
    "#         self.in_dim = len(self.vocab)\n",
    "#         self.start_token = kwargs['start_token']\n",
    "#         self.end_token = kwargs['end_token']\n",
    "#         self.unk_token = kwargs['unk_token']\n",
    "#         self.characters = kwargs['characters']\n",
    "#         self.embed_dim = kwargs['embedding_size']\n",
    "#         self.hid_dim = kwargs['hidden_size']\n",
    "#         self.n_layers = kwargs['num_layers']\n",
    "        \n",
    "#         #Define the embedding layer\n",
    "#         self.embed = nn.Embedding(self.in_dim+1,self.embed_dim,padding_idx=self.in_dim)\n",
    "#         #Define the lstm layer\n",
    "#         self.lstm = nn.LSTM(input_size=self.embed_dim,hidden_size=self.hid_dim,num_layers=self.n_layers)\n",
    "    \n",
    "#     def forward(self, inputs, lengths):\n",
    "#         #Inputs are size (LxBx1)\n",
    "#         #Forward embedding layer\n",
    "#         emb = self.embed(inputs)\n",
    "#         #Embeddings are size (LxBxself.embed_dim)\n",
    "\n",
    "#         #Pack the sequences for GRU\n",
    "#         packed = torch.nn.utils.rnn.pack_padded_sequence(emb, lengths)\n",
    "#         #Forward the GRU\n",
    "#         packed_rec, self.hidden = self.lstm(packed,self.hidden)\n",
    "#         #Unpack the sequences\n",
    "#         rec, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_rec)\n",
    "#         #Hidden outputs are size (LxBxself.hidden_size)\n",
    "        \n",
    "#         #Get last embeddings\n",
    "#         out = rec[lengths-1,list(range(rec.shape[1])),:]\n",
    "#         #Outputs are size (Bxself.hid_dim)\n",
    "        \n",
    "#         return out\n",
    "    \n",
    "#     def init_hidden(self, bsz):\n",
    "#         #Initialise the hidden state\n",
    "#         weight = next(self.parameters())\n",
    "#         self.hidden = (weight.new_zeros(self.n_layers, bsz, self.hid_dim),weight.new_zeros(self.n_layers, bsz, self.hid_dim))\n",
    "\n",
    "#     def detach_hidden(self):\n",
    "#         #Detach the hidden state\n",
    "#         self.hidden=(self.hidden[0].detach(),self.hidden[1].detach())\n",
    "\n",
    "#     def cpu_hidden(self):\n",
    "#         #Set the hidden state to CPU\n",
    "#         self.hidden=(self.hidden[0].detach().cpu(),self.hidden[1].detach().cpu())\n",
    "        \n",
    "# class Predictor(nn.Module):\n",
    "#     def __init__(self, **kwargs):\n",
    "        \n",
    "#         super(Predictor, self).__init__()\n",
    "#         self.hid_dim = kwargs['hidden_size']*2\n",
    "#         self.out_dim = 3\n",
    "#         #Define the output layer and softmax\n",
    "#         self.linear = nn.Linear(self.hid_dim,self.out_dim)\n",
    "#         self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "#     def forward(self,input1,input2):\n",
    "#         #Outputs are size (Bxself.hid_dim)\n",
    "#         inputs = torch.cat((input1,input2),dim=1)\n",
    "#         out = self.softmax(self.linear(inputs))\n",
    "#         return out\n",
    "\n",
    "# def train_model(trainset,trainlabels,encoder,predictor,optimizer,criterion,**kwargs):\n",
    "#     trainlen = trainset.shape[2]\n",
    "#     nbatches = math.ceil(trainlen/kwargs['batch_size'])\n",
    "#     total_loss = 0\n",
    "#     total_backs = 0\n",
    "#     with tqdm(total=nbatches,disable=(kwargs['verbose']<2)) as pbar:\n",
    "#         encoder = encoder.train()\n",
    "#         for b in range(nbatches):\n",
    "#             #Data batch\n",
    "#             X1 = trainset[0,:,b*kwargs['batch_size']:min(trainlen,(b+1)*kwargs['batch_size'])].clone().long().to(kwargs['device'])\n",
    "#             mask1 = torch.clamp(len(kwargs['vocab'])-X1,max=1)\n",
    "#             seq_length1 = torch.sum(mask1,dim=0)\n",
    "#             ordered_seq_length1, dec_index1 = seq_length1.sort(descending=True)\n",
    "#             max_seq_length1 = torch.max(seq_length1)\n",
    "#             X1 = X1[:,dec_index1]\n",
    "#             X1 = X1[0:max_seq_length1]\n",
    "#             rev_dec_index1 = list(range(seq_length1.shape[0]))\n",
    "#             for i,j in enumerate(dec_index1):\n",
    "#                 rev_dec_index1[j] = i\n",
    "#             X2 = trainset[1,:,b*kwargs['batch_size']:min(trainlen,(b+1)*kwargs['batch_size'])].clone().long().to(kwargs['device'])\n",
    "#             mask2 = torch.clamp(len(kwargs['vocab'])-X2,max=1)\n",
    "#             seq_length2 = torch.sum(mask2,dim=0)\n",
    "#             ordered_seq_length2, dec_index2 = seq_length2.sort(descending=True)\n",
    "#             max_seq_length2 = torch.max(seq_length2)\n",
    "#             X2 = X2[:,dec_index2]\n",
    "#             X2 = X2[0:max_seq_length2]\n",
    "#             rev_dec_index2 = list(range(seq_length2.shape[0]))\n",
    "#             for i,j in enumerate(dec_index2):\n",
    "#                 rev_dec_index2[j] = i\n",
    "#             Y = trainlabels[b*kwargs['batch_size']:min(trainlen,(b+1)*kwargs['batch_size'])].clone().long().to(kwargs['device'])\n",
    "#             #Forward pass\n",
    "#             encoder.init_hidden(X1.size(1))\n",
    "#             embeddings1 = encoder(X1,ordered_seq_length1)\n",
    "#             encoder.detach_hidden()\n",
    "#             encoder.init_hidden(X2.size(1))\n",
    "#             embeddings2 = encoder(X2,ordered_seq_length2)\n",
    "#             embeddings1 = embeddings1[rev_dec_index1]\n",
    "#             embeddings2 = embeddings2[rev_dec_index2]\n",
    "#             posteriors = predictor(embeddings1,embeddings2)\n",
    "#             loss = criterion(posteriors,Y)\n",
    "#             #Backpropagate\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             #Estimate the latest loss\n",
    "#             if total_backs == 100:\n",
    "#                 total_loss = total_loss*0.99+loss.detach().cpu().numpy()\n",
    "#             else:\n",
    "#                 total_loss += loss.detach().cpu().numpy()\n",
    "#                 total_backs += 1\n",
    "#             encoder.detach_hidden()\n",
    "#             pbar.set_description(f'Training epoch. Loss {total_loss/(total_backs+1):.2f}')\n",
    "#             pbar.update()\n",
    "#     return total_loss/(total_backs+1)\n",
    "\n",
    "# def evaluate_model(testset,encoder,predictor,**kwargs):\n",
    "#     testlen = testset.shape[2]\n",
    "#     nbatches = math.ceil(testlen/kwargs['batch_size'])\n",
    "#     predictions = np.zeros((testlen,))\n",
    "#     with torch.no_grad():\n",
    "#         encoder = encoder.eval()\n",
    "#         for b in range(nbatches):\n",
    "#             #Data batch\n",
    "#             X1 = testset[0,:,b*kwargs['batch_size']:min(testlen,(b+1)*kwargs['batch_size'])].clone().long().to(kwargs['device'])\n",
    "#             mask1 = torch.clamp(len(kwargs['vocab'])-X1,max=1)\n",
    "#             seq_length1 = torch.sum(mask1,dim=0)\n",
    "#             ordered_seq_length1, dec_index1 = seq_length1.sort(descending=True)\n",
    "#             max_seq_length1 = torch.max(seq_length1)\n",
    "#             X1 = X1[:,dec_index1]\n",
    "#             X1 = X1[0:max_seq_length1]\n",
    "#             rev_dec_index1 = list(range(seq_length1.shape[0]))\n",
    "#             for i,j in enumerate(dec_index1):\n",
    "#                 rev_dec_index1[j] = i\n",
    "#             X2 = testset[1,:,b*kwargs['batch_size']:min(testlen,(b+1)*kwargs['batch_size'])].clone().long().to(kwargs['device'])\n",
    "#             mask2 = torch.clamp(len(kwargs['vocab'])-X2,max=1)\n",
    "#             seq_length2 = torch.sum(mask2,dim=0)\n",
    "#             ordered_seq_length2, dec_index2 = seq_length2.sort(descending=True)\n",
    "#             max_seq_length2 = torch.max(seq_length2)\n",
    "#             X2 = X2[:,dec_index2]\n",
    "#             X2 = X2[0:max_seq_length2]\n",
    "#             rev_dec_index2 = list(range(seq_length2.shape[0]))\n",
    "#             for i,j in enumerate(dec_index2):\n",
    "#                 rev_dec_index2[j] = i\n",
    "#             #Forward pass\n",
    "#             encoder.init_hidden(X1.size(1))\n",
    "#             embeddings1 = encoder(X1,ordered_seq_length1)\n",
    "#             encoder.init_hidden(X2.size(1))\n",
    "#             embeddings2 = encoder(X2,ordered_seq_length2)\n",
    "#             embeddings1 = embeddings1[rev_dec_index1]\n",
    "#             embeddings2 = embeddings2[rev_dec_index2]\n",
    "#             posteriors = predictor(embeddings1,embeddings2)\n",
    "#             #posteriors = model(X,ordered_seq_length)\n",
    "#             estimated = torch.argmax(posteriors,dim=1)\n",
    "#             predictions[b*kwargs['batch_size']:min(testlen,(b+1)*kwargs['batch_size'])] = estimated.detach().cpu().numpy()\n",
    "#     return predictions\n",
    "        \n",
    "# #Arguments\n",
    "# args = {\n",
    "#     'cv_percentage': 0.1,\n",
    "#     'epochs': 20,\n",
    "#     'batch_size': 128,\n",
    "#     'embedding_size': 16,\n",
    "#     'hidden_size': 64,\n",
    "#     'num_layers': 1,\n",
    "#     'learning_rate': 0.01,\n",
    "#     'seed': 0,\n",
    "#     'start_token': '<s>',\n",
    "#     'end_token': '<\\s>',\n",
    "#     'unk_token': '<UNK>',\n",
    "#     'verbose': 1,\n",
    "#     'characters': False,\n",
    "#     'min_count': 15,\n",
    "#     'device': torch.device(('cuda:0' if torch.cuda.is_available() else 'cpu'))\n",
    "#     }\n",
    "\n",
    "# #Read data\n",
    "# train_data = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/train.csv')\n",
    "# test_data = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/test.csv')\n",
    "# #Extract only English language cases\n",
    "# train_data = train_data.loc[train_data['language']=='English']\n",
    "# test_data = test_data.loc[test_data['language']=='English']\n",
    "# #Extract premises and hypothesis\n",
    "# train_premise = [normalise(v) for v in train_data.premise.values]\n",
    "# train_hypothesis = [normalise(v) for v in train_data.hypothesis.values]\n",
    "# test_premise = [normalise(v) for v in test_data.premise.values]\n",
    "# test_hypothesis = [normalise(v) for v in test_data.hypothesis.values]\n",
    "# train_targets = train_data.label.values\n",
    "# print('Training: {0:d} pairs in English. Evaluation: {1:d} pairs in English'.format(len(train_premise),len(test_premise)))\n",
    "# print('Label distribution in training set: {0:s}'.format(str({i:'{0:.2f}%'.format(100*len(np.where(train_targets==i)[0])/len(train_targets)) for i in [0,1,2]})))\n",
    "\n",
    "# batch_sizes = [64,128,256]\n",
    "# min_counts = [5,15,25]\n",
    "\n",
    "# it_idx = 0\n",
    "# valid_predictions = dict()\n",
    "# test_predictions = dict()\n",
    "# valid_accuracies = dict()\n",
    "\n",
    "# for batch_size in batch_sizes:\n",
    "#     for min_count in min_counts:\n",
    "#         args['batch_size'] = batch_size\n",
    "#         args['min_count'] = min_count\n",
    "    \n",
    "#         random_init(**args)\n",
    "\n",
    "#         #Make vocabulary and load data\n",
    "#         args['vocab'] = read_vocabulary(train_premise+train_hypothesis, **args)\n",
    "#         #print('Vocabulary size: {0:d} tokens'.format(len(args['vocab'])))\n",
    "#         trainset, validset, trainlabels, validlabels = load_data(train_premise, train_hypothesis, train_targets, cv=True, **args)\n",
    "#         testset, _ = load_data(test_premise, test_hypothesis, None, cv=False, **args)\n",
    "\n",
    "#         #Create model, optimiser and criterion\n",
    "#         encoder = LSTMEncoder(**args).to(args['device'])\n",
    "#         predictor = Predictor(**args).to(args['device'])\n",
    "#         optimizer = torch.optim.Adam(list(encoder.parameters())+list(predictor.parameters()),lr=args['learning_rate'])\n",
    "#         criterion = nn.NLLLoss(reduction='mean').to(args['device'])\n",
    "\n",
    "#         #Train epochs\n",
    "#         best_acc = 0.0\n",
    "#         for ep in range(1,args['epochs']+1):\n",
    "#             loss = train_model(trainset,trainlabels,encoder,predictor,optimizer,criterion,**args)\n",
    "#             val_pred = evaluate_model(validset,encoder,predictor,**args)\n",
    "#             test_pred = evaluate_model(testset,encoder,predictor,**args)\n",
    "#             acc = 100*len(np.where((val_pred-validlabels.numpy())==0)[0])/validset.shape[2]\n",
    "#             if acc >= best_acc:\n",
    "#                 best_acc = acc\n",
    "#                 best_epoch = ep\n",
    "#                 best_loss = loss\n",
    "#                 valid_predictions[it_idx] = val_pred\n",
    "#                 valid_accuracies[it_idx] = acc\n",
    "#                 test_predictions[it_idx] = test_pred\n",
    "#         print('Run {0:d}. Best epoch: {1:d} of {2:d}. Training loss: {3:.2f}, validation accuracy: {4:.2f}%, test label distribution: {5:s}'.format(it_idx+1,best_epoch,args['epochs'],best_loss,best_acc,str({i:'{0:.2f}%'.format(100*len(np.where(test_pred==i)[0])/len(test_pred)) for i in [0,1,2]})))\n",
    "#         it_idx += 1\n",
    "\n",
    "# #Do the score combination\n",
    "# best_epochs = np.argsort([valid_accuracies[ep] for ep in range(it_idx)])[::-1]\n",
    "# val_pred = np.array([valid_predictions[ep] for ep in best_epochs[0:5]])\n",
    "# val_pred = np.argmax(np.array([np.sum((val_pred==i).astype(int),axis=0) for i in [0,1,2]]),axis=0)\n",
    "# test_pred = np.array([test_predictions[ep] for ep in best_epochs[0:5]])\n",
    "# test_pred = np.argmax(np.array([np.sum((test_pred==i).astype(int),axis=0) for i in [0,1,2]]),axis=0)\n",
    "# acc = 100*len(np.where((val_pred-validlabels.numpy())==0)[0])/validset.shape[2]\n",
    "# print('Ensemble. Cross-validation accuracy: {0:.2f}%, test label distribution: {1:s}'.format(acc,str({i:'{0:.2f}%'.format(100*len(np.where(test_pred==i)[0])/len(test_pred)) for i in [0,1,2]})))\n",
    "# #Set all predictions to the majority category\n",
    "# df_out = pd.DataFrame({'id': pd.read_csv('/kaggle/input/contradictory-my-dear-watson/test.csv')['id'], 'prediction': np.argmax([len(np.where(train_targets==i)[0]) for i in [0,1,2]])})\n",
    "# #Set only English language cases to the predicted labels\n",
    "# df_out.loc[df_out['id'].isin(test_data['id']),'prediction']=test_pred\n",
    "# df_out.to_csv('/kaggle/working/submission.csv'.format(it_idx,acc),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

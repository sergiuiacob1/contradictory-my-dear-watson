{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from tensorflow.python.framework.ops import disable_eager_execution, enable_eager_execution\nenable_eager_execution()\n\nimport os\nimport numpy as np\nimport pandas as pd\nfrom transformers import BertTokenizer, TFBertModel\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# tensorflow imports\nfrom tensorflow import int32\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.backend import placeholder\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.layers import Embedding, Dense\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import LSTM, Bidirectional\nfrom tensorflow.keras.layers import Flatten, Softmax\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import Input, Model\nfrom tensorflow.keras.layers import Activation, Add\nimport tensorflow.keras.backend as K\nimport tensorflow as tf\nimport re\nimport nltk\nfrom gensim.models import word2vec\n\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n    print('Number of replicas:', strategy.num_replicas_in_sync)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-13T18:56:49.453393Z","iopub.execute_input":"2022-01-13T18:56:49.453682Z","iopub.status.idle":"2022-01-13T18:56:49.475847Z","shell.execute_reply.started":"2022-01-13T18:56:49.453652Z","shell.execute_reply":"2022-01-13T18:56:49.474923Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/contradictory-my-dear-watson/train.csv\")\ntest_df = pd.read_csv(\"../input/contradictory-my-dear-watson/test.csv\")\nlen(train_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T18:56:49.478149Z","iopub.execute_input":"2022-01-13T18:56:49.478764Z","iopub.status.idle":"2022-01-13T18:56:49.609927Z","shell.execute_reply.started":"2022-01-13T18:56:49.478718Z","shell.execute_reply":"2022-01-13T18:56:49.608974Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"train_df = train_df[train_df['language'] == 'English']\ntest_df = test_df[test_df['language'] == 'English']\nlen(train_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T18:56:49.611255Z","iopub.execute_input":"2022-01-13T18:56:49.612445Z","iopub.status.idle":"2022-01-13T18:56:49.628478Z","shell.execute_reply.started":"2022-01-13T18:56:49.612394Z","shell.execute_reply":"2022-01-13T18:56:49.627535Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"temp = pd.DataFrame()\ntemp['premise'] = train_df['premise']\ntemp['hypothesis'] = train_df['hypothesis']","metadata":{"execution":{"iopub.status.busy":"2022-01-13T18:56:49.629822Z","iopub.execute_input":"2022-01-13T18:56:49.630624Z","iopub.status.idle":"2022-01-13T18:56:49.640412Z","shell.execute_reply.started":"2022-01-13T18:56:49.630587Z","shell.execute_reply":"2022-01-13T18:56:49.639694Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2022-01-13T18:56:49.642514Z","iopub.execute_input":"2022-01-13T18:56:49.642882Z","iopub.status.idle":"2022-01-13T18:56:49.655009Z","shell.execute_reply.started":"2022-01-13T18:56:49.642850Z","shell.execute_reply":"2022-01-13T18:56:49.654352Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"STOP_WORDS = nltk.corpus.stopwords.words()\n\ndef clean_sentence(val):\n    regex = re.compile('([^\\s\\w]|_)+')\n    sentence = regex.sub('', val).lower()\n    sentence = sentence.split(\" \")\n    \n    for word in list(sentence):\n        if word in STOP_WORDS:\n            sentence.remove(word)  \n            \n    sentence = \" \".join(sentence)\n    return sentence\n\ntemp['premise'] =  temp['premise'].apply(clean_sentence)\ntemp['hypothesis'] =  temp['hypothesis'].apply(clean_sentence)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-13T18:56:49.656224Z","iopub.execute_input":"2022-01-13T18:56:49.656749Z","iopub.status.idle":"2022-01-13T18:57:03.139047Z","shell.execute_reply.started":"2022-01-13T18:56:49.656718Z","shell.execute_reply":"2022-01-13T18:57:03.138300Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def build_corpus(data):\n    corpus = []\n    for col in ['premise', 'hypothesis']:\n        for sentence in data[col].iteritems():\n            word_list = sentence[1].split(\" \")\n            corpus.append(word_list)\n            \n    return corpus\n\ncorpus = build_corpus(temp)        \n","metadata":{"execution":{"iopub.status.busy":"2022-01-13T18:57:03.140138Z","iopub.execute_input":"2022-01-13T18:57:03.140367Z","iopub.status.idle":"2022-01-13T18:57:03.171571Z","shell.execute_reply.started":"2022-01-13T18:57:03.140338Z","shell.execute_reply":"2022-01-13T18:57:03.170640Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def tsne_plot(model, word_limit=100):\n    \"Creates and TSNE model and plots it\"\n    labels = []\n    tokens = []\n\n    for word in list(model.wv.key_to_index)[:word_limit]:\n        tokens.append(model.wv[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=(16, 16)) \n    for i in range(len(x)):\n        plt.scatter(x[i],y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-01-13T18:57:03.172800Z","iopub.execute_input":"2022-01-13T18:57:03.173768Z","iopub.status.idle":"2022-01-13T18:57:03.183869Z","shell.execute_reply.started":"2022-01-13T18:57:03.173731Z","shell.execute_reply":"2022-01-13T18:57:03.183213Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"encoder = word2vec.Word2Vec(corpus, vector_size=100, window=20, min_count=2, workers=8)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T18:57:03.184823Z","iopub.execute_input":"2022-01-13T18:57:03.185070Z","iopub.status.idle":"2022-01-13T18:57:04.371100Z","shell.execute_reply.started":"2022-01-13T18:57:03.185030Z","shell.execute_reply":"2022-01-13T18:57:04.370300Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"tsne_plot(encoder, word_limit=1000)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T18:57:04.373346Z","iopub.execute_input":"2022-01-13T18:57:04.373664Z","iopub.status.idle":"2022-01-13T18:57:40.319228Z","shell.execute_reply.started":"2022-01-13T18:57:04.373621Z","shell.execute_reply":"2022-01-13T18:57:40.318360Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"result = []\nfor word in np.concatenate(corpus):\n    if word in encoder.wv:\n        result.append(1)\n    else:\n        result.append(0)\n        \nprint(\"%d/%d\" % (sum(result), len(result)))\n\ndef encode_f(sentence):\n    words = sentence.lower().split()\n    result = []\n    for word in words:\n        if word in encoder.wv:\n            result.append(encoder.wv[word])\n    return result\n\ntrain_df.premise = train_df.premise.apply(encode_f)\ntrain_df.hypothesis = train_df.hypothesis.apply(encode_f)\n\ntest_df.premise = test_df.premise.apply(encode_f)\ntest_df.hypothesis = test_df.hypothesis.apply(encode_f)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T18:57:40.320846Z","iopub.execute_input":"2022-01-13T18:57:40.321079Z","iopub.status.idle":"2022-01-13T18:57:41.231148Z","shell.execute_reply.started":"2022-01-13T18:57:40.321050Z","shell.execute_reply":"2022-01-13T18:57:41.230319Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntrain_x1 = pad_sequences(train_df.premise.to_numpy(), dtype='float', maxlen=70)\ntrain_x2 = pad_sequences(train_df.hypothesis.to_numpy(), dtype='float', maxlen=70)\ntrain_y = train_df.label.to_numpy()\n\ntest_x1 = pad_sequences(test_df.premise.to_numpy(), dtype='float', maxlen=70)\ntest_x2 = pad_sequences(test_df.hypothesis.to_numpy(), dtype='float', maxlen=70)","metadata":{"execution":{"iopub.status.busy":"2022-01-13T18:57:41.232396Z","iopub.execute_input":"2022-01-13T18:57:41.232858Z","iopub.status.idle":"2022-01-13T18:57:41.871247Z","shell.execute_reply.started":"2022-01-13T18:57:41.232822Z","shell.execute_reply":"2022-01-13T18:57:41.870336Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"print(train_df.premise.to_numpy()[0])\nprint(train_x1[0])","metadata":{"execution":{"iopub.status.busy":"2022-01-13T18:57:41.872475Z","iopub.execute_input":"2022-01-13T18:57:41.872682Z","iopub.status.idle":"2022-01-13T18:57:41.885498Z","shell.execute_reply.started":"2022-01-13T18:57:41.872655Z","shell.execute_reply":"2022-01-13T18:57:41.884329Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"For the Actual LSTM implementation, we will be using a Siamese BiLSTM network model described in [this](https://github.com/GKarmakar/siamese-lstm/blob/master/docs/W16-1617.pdf) paper by Paul Necoiu, Maarten Versteegh and Mihai Rotaru. A base implementation can also be found by GKarmakar [here](https://github.com/GKarmakar/deep-siamese-text-similarity/blob/master/siamese_network_semantic.py). While we base our approach on these two previous works, we propose a number of adjustments to the model, and also define a more abstract setup to allow fine tuning and exploration.\n\nFirstly, here is the SiameseBiLSTM implementation. This is going to be the core of the model, consisting of the two BiLSTM RNNs that parse the hypothesis and the premise respectively, together with the energy function that produces the final output based on the RNN outputs.","metadata":{}},{"cell_type":"code","source":"class SiameseBiLSTM:\n    args = None\n    ff_layer = None\n\n    def __init__(self, args):\n        self.args = args\n        self.define_bilstm()\n\n        self.input_p1 = placeholder(dtype=int32, shape=[None, args['max_sentence_size']], name='input_p1')\n        self.input_p2 = placeholder(dtype=int32, shape=[None, args['max_sentence_size']], name='input_p2')\n        self.input_y = placeholder(dtype=int32, shape=[None], name='input_y')\n\n    def define_bilstm(self):\n        \"\"\"\n        Defines a BiLSTM network that will be used to handle one of the input sentences. There will be two\n        such networks in the final model, one for the premise and one for the hypothesis.\n        \"\"\"\n\n        # n layers of BiLSTM RNNs of size 64x2\n        feed_forward_layer = []\n        for num_units in self.args[\"num_units\"][:-1]:\n            # This will use tanh as the default activation\n            feed_forward_layer.append(Bidirectional(\n                LSTM(num_units,\n                     activation='tanh',\n                     recurrent_activation='sigmoid',\n#                      dropout=0.0,\n#                      recurrent_dropout=0.05,\n#                      kernel_regularizer=regularizers.l2(0.03),\n                     return_sequences=True)))\n\n        self.ff_layer = feed_forward_layer\n\n        self.top_layer = LSTM(self.args[\"num_units\"][-1],\n                                  activation='hard_sigmoid',\n#                                   dropout=0.0,\n#                                   recurrent_dropout=0.05,\n                                  kernel_regularizer=regularizers.l2(0.03),\n                                  return_sequences=True)\n\n    @staticmethod\n    def energy_function(a, b):\n        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n\n\nsiamese_bilstm = SiameseBiLSTM({\n    'num_units': [512, 256, 128, 128],\n    'ff_layers': 3,\n    'max_sentence_size': 70\n})\n\n","metadata":{"execution":{"iopub.status.busy":"2022-01-13T18:57:41.888877Z","iopub.execute_input":"2022-01-13T18:57:41.889221Z","iopub.status.idle":"2022-01-13T18:57:41.939632Z","shell.execute_reply.started":"2022-01-13T18:57:41.889183Z","shell.execute_reply":"2022-01-13T18:57:41.938681Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"--- to complete dropout, train/test shuffler and l2 optimization---\n\nFinally, the model that puts all of these components together to be able to train and test","metadata":{}},{"cell_type":"code","source":"class Diff(Add):\n    def _merge_function(self, inputs):\n        return K.sum(K.abs(inputs[1] - inputs[0]), axis=-1, keepdims=True)\n\n    # def _compute_elemwise_op_output_shape(self, shape1, shape2):\n    #     return None, None, 1\n\n    def compute_output_shape(self, input_shape):\n        output_shape = (1,)\n\n        batch_sizes = [s[0] for s in input_shape if s is not None]\n        batch_sizes = set(batch_sizes)\n        batch_sizes -= {None}\n        if len(batch_sizes) == 1:\n            output_shape = (list(batch_sizes)[0],) + output_shape\n        else:\n            output_shape = (None,) + output_shape\n        return output_shape\n\n\nclass CosineDist(Diff):\n    def _merge_function(self, inputs):\n        l, r = inputs[0], inputs[1]\n        num = K.sum((l * r), keepdims=True, axis=-1)\n        den = K.sqrt(K.sum(K.square(l), keepdims=True, axis=-1)) * K.sqrt(K.sum(K.square(r), keepdims=True, axis=-1))\n        den = K.clip(den, min_value=1e-4, max_value=float('inf'))\n        sim = num / den\n        return K.ones_like(sim) - sim\n\n    def _compute_elemwise_op_output_shape(self, shape1, shape2):\n        return None, 1\n\n    def compute_output_shape(self, input_shape):\n        return None, 1\n    \ndef mean_rectified_infinity_loss(y_true, y_pred):\n    k = 5.0\n\n    cond = K.equal(y_true, K.zeros_like(y_true))\n    if K.backend() == 'tensorflow':\n        import tensorflow as tf\n        # err = tf.where(cond, K.square(y_pred - y_true), K.exp(-y_pred / k))\n        err = tf.where(cond, K.square(y_pred),\n                       k/K.square(K.clip(y_pred, min_value=K.epsilon(), max_value=float('inf'))))\n    else:\n        from theano.ifelse import ifelse\n        # err = ifelse(cond, K.square(y_pred - y_true), K.exp(-y_pred / k))\n        err = ifelse(cond, K.square(y_pred),\n                     k/K.square(K.clip(y_pred, min_value=K.epsilon(), max_value=float('inf'))))\n\n    return K.mean(err, axis=-1)\n\ndef contrastive_loss(y, preds, margin=1):\n    y = tf.cast(y, preds.dtype)\n\n    squaredPreds = K.square(preds)\n    squaredMargin = K.square(K.maximum(margin - preds, 0))\n    loss = K.mean(y * squaredPreds + (1 - y) * squaredMargin)\n    return loss\n\nclass ContradictoryModel:\n\n    def __init__(self, _encoder, data_handler, rnn, args):\n        self.args = args\n        self.encoder = _encoder\n        self.data_handler = data_handler\n        self.rnn = rnn\n        self.optimizer = None\n        self.loss_function = contrastive_loss\n        self.sequence = Sequential()\n        additional_metrics = ['accuracy']\n        # self.sequence.add(Embedding(args['num_distinct_words'],\n        #                          args['embedding_output_dims'],\n        #                          input_length=args['max_sequence_length']))\n        for layer in self.rnn.ff_layer:\n            self.sequence.add(layer)\n#         self.sequence.add(self.rnn.top_layer)\n        self.sequence.add(Dense(128, activation='tanh', kernel_regularizer=regularizers.l2(0.03)))\n        \n        p1_in = Input((70, 100), name=\"p1input\")\n        p1_input = self.sequence(p1_in)\n        p2_in = Input((70, 100), name=\"p2input\")\n        p2_input = self.sequence(p2_in)\n\n        merged = CosineDist(name='merge')([p1_input, p2_input])\n        out = Activation('relu', name='out')(merged)\n        out = Flatten()(out)\n        out = Dense(3)(out)\n        out = Softmax()(out)\n\n        self.model = Model(inputs=(p1_in, p2_in), outputs=out)\n\n        self.model.compile(loss=self.loss_function, metrics=additional_metrics)\n        print(\"Model compiled successfully.\")\n        self.sequence.summary()\n\n    def fit(self, x1, x2, y):\n        history = self.model.fit([x1, x2], y,\n                                    batch_size=self.args['batch_size'],\n                                    epochs=self.args['number_of_epochs'],\n                                    verbose=1, validation_split=self.args['validation_split'])\n\n    def test(self, x1, x2, y):\n        test_results = self.model.evaluate([x1, x2], y, verbose=False)\n        print(f'Test results - Loss: {test_results[0]} - Accuracy: {100 * test_results[1]}%')\n\n    def predict(self, p1, p2):\n        x1, x2 = pad_sequences([encode_f(p1), encode_f(p2)], dtype='float', maxlen=70)\n        x1 = x1.reshape([1, 70, 100])\n        x2 = x2.reshape([1, 70, 100])\n\n        print(self.model.predict([x1, x2]))\n\n\n\nmodel = ContradictoryModel(encoder, None, siamese_bilstm, {\n    'num_distinct_words': 1000,\n    'embedding_output_dims': 64,\n    'max_sequence_length': 100,\n    'batch_size': 128,\n    'validation_split': 0.2,\n    'number_of_epochs': 1\n})\n\nmodel.fit(train_x1, train_x2, train_y)\nmodel.predict(\"Steps are initiated to allow program board membership to reflect the clienteligible community and include representatives from the funding community, corporations and other partners.\", \"There's enough room for 35-40 positions on the board.\")\n# model.test(train_x1[5501:], train_x2[5501:], train_y[5501:])\n","metadata":{"execution":{"iopub.status.busy":"2022-01-13T18:57:41.941544Z","iopub.execute_input":"2022-01-13T18:57:41.941870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}